{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOWkGH/hQfEFzavxhhUcJFb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ieg-dhr/NLP-Course4Humanities_2024/blob/main/Data_Detective_NLP_Introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Detective: Uncovering Hidden Themes using NLP tasks\n",
        "\n",
        "Created by Sarah Oberbichler [![ORCID](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyMCIgaGVpZ2h0PSIyMCIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8cmVjdCB3aWR0aD0iMjAiIGhlaWdodD0iMjAiIGZpbGw9IiNGRkZGRkYiLz4KICA8Y2lyY2xlIGN4PSIxMCIgY3k9IjEwIiByPSI5IiBmaWxsPSIjQThDRTNDIi8+CiAgPHRleHQgeD0iMTAiIHk9IjE1IiBmb250LWZhbWlseT0iQXJpYWwsIHNhbnMtc2VyaWYiIGZvbnQtc2l6ZT0iMTEiIGZvbnQtd2VpZ2h0PSJib2xkIiB0ZXh0LWFuY2hvcj0ibWlkZGxlIiBmaWxsPSIjRkZGRkZGIj5pRDwvdGV4dD4KPC9zdmc+)](https://orcid.org/0000-0002-1031-2759)\n"
      ],
      "metadata": {
        "id": "nS9UP1tFrJyV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Welcome to this digital investigation lab! Today, we're facing an intriguing challenge:\n",
        "\n",
        "**Our Dataset:**\n",
        "A collection of newspaper pages, each containing various articles. Hidden within these pages is a common theme, connecting all the content.\n",
        "\n",
        "**The Challenge:**\n",
        "Reading through all these articles manually would be a time-consuming task, potentially taking hours or even days. Yet, somewhere in this sea of words lies our answer.\n",
        "\n",
        "**Our Approach:**\n",
        "Instead of traditional reading, we'll employ Natural Language Processing (NLP) techniques. These computational methods will help us analyze the text efficiently and uncover patterns that might not be immediately obvious to the human eye.\n",
        "\n",
        "**Our Mission:**\n",
        "Use basic NLP tools to identify the common thread running through these newspaper articles. We'll walk through this process step by step, uncovering insights along the way.\n",
        "\n",
        "Ready to start our text analysis journey? Let's see what stories our data can tell us!"
      ],
      "metadata": {
        "id": "TsKoSWc61dGV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing the Dataset to the Notebook\n",
        "\n",
        "In order to access our course data, we clone the course GitHub repository to this notebook. Do do so, run the *git clone* cell below:"
      ],
      "metadata": {
        "id": "nqr-KFxyrxz9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHFu_RzArIrL"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/ieg-dhr/NLP-Course4Humanities_2024.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Replace 'your_file.xlsx' with the actual path to your Excel file.\n",
        "df = pd.read_excel('/content/NLP-Course4Humanities_2024/datasets/Data_exercise_1.xlsx')\n",
        "\n",
        "# Display the first few rows of the DataFrame to verify it's loaded correctly.\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "KrvRQj4Qr8XU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleaning the Dataset\n",
        "\n",
        "Before we can start applying NLP methods to the dataset, we need to prepare the text in a way that the machine has to deal with less \"noise\". Less noise means punctuation marks and special characters are removed and all words are uniformly written in lowercase."
      ],
      "metadata": {
        "id": "03SILs8osv0p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "#Function to clean\n",
        "\n",
        "def initial_clean(text):\n",
        "    text = re.sub(r'[^\\w\\s]','',text)\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "df['cleaned'] = df['plainpagefulltext'].apply(initial_clean)\n",
        "df['cleaned'][:5]"
      ],
      "metadata": {
        "id": "DyPZttjOtXX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Basic NLP task: Tokenization\n",
        "\n",
        "Tokenization is the process of breaking down text into smaller units called tokens, typically words or subwords. It's a fundamental step in natural language processing that enables machines to analyze and understand text by converting it into a format they can process more easily."
      ],
      "metadata": {
        "id": "04sPSHEVp6KR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "def tokenize(text):\n",
        "  text = nltk.word_tokenize(text, language = 'german')\n",
        "  return text\n",
        "\n",
        "#continue the code\n",
        "df['tokenized'] = df['cleaned'].apply(tokenize)\n",
        "df['tokenized'][:5]"
      ],
      "metadata": {
        "id": "FYG5IWMmp_w_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Cloud\n",
        "Let's visualize this bag of words and see if we already can detect our common theme"
      ],
      "metadata": {
        "id": "JRj-v2spRND2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Flatten the list of lists of lemmatized words into a single list\n",
        "all_words = [word for sublist in df['tokenized'] for word in sublist]\n",
        "\n",
        "# Create a string of all words\n",
        "text = ' '.join(all_words)\n",
        "\n",
        "# Generate the word cloud\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "\n",
        "# Display the generated image:\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WLbMAIKW1hwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  2. Basic NLP task: Lemmatization and Stop Words Removal\n",
        "\n",
        "The lexicographic reduction of inflected forms of a word to a base form, that is, the determination of the base form of a lexeme and the arrangement of the lemmas, is also called lemmatization."
      ],
      "metadata": {
        "id": "JWVXJ6WdKtKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Ensure the German language model is downloaded\n",
        "!python -m spacy download de_core_news_sm\n",
        "\n",
        "# Load the German language model\n",
        "nlp = spacy.load('de_core_news_sm')\n",
        "\n",
        "# Get German stop words and add custom ones\n",
        "stop_words = set(stopwords.words('german'))\n",
        "custom_stop_words = {'herr', 'frau', 'dez', 'januar', 'ge', 'nr', 'Å¿ind', 'handeln'}\n",
        "stop_words.update(custom_stop_words)\n",
        "\n",
        "def lemmatize_and_remove_stopwords(texts):\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        # Join the tokens into a single string\n",
        "        text = \" \".join(sent)\n",
        "\n",
        "\n",
        "        # Process the text with spaCy\n",
        "        doc = nlp(text)\n",
        "\n",
        "        # Lemmatize, lowercase, and remove stop words\n",
        "        lemmatized = [token.lemma_.lower() for token in doc\n",
        "                      if token.is_alpha and token.lemma_.lower() not in stop_words]\n",
        "\n",
        "        texts_out.append(lemmatized)\n",
        "\n",
        "    return texts_out\n",
        "\n",
        "# Apply the function to the DataFrame\n",
        "df['lemmatized'] = lemmatize_and_remove_stopwords(df['tokenized'])\n",
        "\n",
        "# Display the first 5 rows of the lemmatized column\n",
        "df['lemmatized'].head()\n"
      ],
      "metadata": {
        "id": "YBFAOsPlwrcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Let's visualize the results"
      ],
      "metadata": {
        "id": "0knT9lVn26mL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Flatten the list of lists of lemmatized words into a single list\n",
        "all_words = [word for sublist in df['lemmatized'] for word in sublist]\n",
        "\n",
        "# Create a string of all words\n",
        "text = ' '.join(all_words)\n",
        "\n",
        "# Generate the word cloud\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "\n",
        "# Display the generated image:\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eSO89w2Z48fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unfortunaltey, this word cloud does not yet give a meaningful insight into our dataset. Since many verbs and adjectives are very repetitive, those are also the most frequent in the data. Let's try to process the data further in order to focus on words that carry more meaning.\n",
        "\n",
        "Can you guess what part of speech carries more meaning when it comes to the detection of common themes?"
      ],
      "metadata": {
        "id": "4aart1LxasAD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Basic NLP Task: Part of Speech Tagging\n",
        "Part-of-speech tagging (POS tagging) refers to the process of assigning words and punctuation marks in a text to their corresponding parts of speech (word classes). This process takes into account both the definition of the word and its context (e.g., adjacent adjectives or nouns)."
      ],
      "metadata": {
        "id": "7lvVeUUZ5pw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tagging(texts, allowed_postags=['NOUN']): # possible tags'NOUN', 'ADJ', 'ADV', 'VERB'\n",
        "    texts_out = []\n",
        "    nlp = spacy.load('de_core_news_sm')\n",
        "    for sent in texts:\n",
        "        sent_str = \" \".join(sent)\n",
        "        doc = nlp(sent_str)\n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    return texts_out\n",
        "\n",
        "df['tagging'] = tagging(df['lemmatized'])\n",
        "df['tagging'][:5]"
      ],
      "metadata": {
        "id": "Ohtq93LO53aP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Let's visualize the results"
      ],
      "metadata": {
        "id": "UkTLuZN23Imi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Flatten the list of lists of lemmatized words into a single list\n",
        "all_words = [word for sublist in df['tagging'] for word in sublist]\n",
        "\n",
        "# Create a string of all words\n",
        "text = ' '.join(all_words)\n",
        "\n",
        "# Generate the word cloud\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "\n",
        "# Display the generated image:\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ULz1TCBc72mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems like we are still not able to detect any common theme.\n",
        "\n",
        "Any idea how we could solve this problem?"
      ],
      "metadata": {
        "id": "7YRLxHR1c3LW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Basic NLP task: TF IDF Vectorizer\n",
        "\n",
        "The problem with counting word occurrences is that some words are less frequent but important in a relation to a collection of documents.\n",
        "\n",
        "For this reason, it is sometimes better to normalize the word counts by the number of times they appear in the documents. This is the general idea behind the tf-idf vectorization.\n",
        "\n",
        "Tf stands for **term frequency**, the number of times the word appears in each document. We already did this before.\n",
        "\n",
        "Idf stands for **inverse document frequency**, an inverse count of the number of documents a word appears in. Idf measures how significant a word is in the whole corpus.\n",
        "\n"
      ],
      "metadata": {
        "id": "1yUoFmzU3L0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "# Convert the lists in 'tagging' column to strings\n",
        "df['lemmatized_str'] = df['lemmatized'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# Create and fit the TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=50000)  # You can adjust max_features as needed\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['lemmatized_str'])\n",
        "\n",
        "# Get feature names (words) and their TF-IDF scores\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "tfidf_scores = tfidf_matrix.sum(axis=0).A1\n",
        "print(\"Feature Names:\", feature_names)\n",
        "print(\"TF-IDF Scores:\", tfidf_scores)\n",
        "\n",
        "# Create a DataFrame with words and their TF-IDF scores\n",
        "word_tfidf_df = pd.DataFrame({'word': feature_names, 'tfidf_score': tfidf_scores})\n",
        "\n",
        "# Sort by TF-IDF score in descending order (most frequent/important words first)\n",
        "word_tfidf_df = word_tfidf_df.sort_values('tfidf_score', ascending=False)\n",
        "\n",
        "# Select top N words (you can adjust this number)\n",
        "top_n = 1000\n",
        "top_words = word_tfidf_df['word'].head(top_n).tolist()\n",
        "\n",
        "# Create the 'j' column in the original DataFrame\n",
        "df['vectorized'] = df['tagging'].apply(lambda x: [word for word in x if word in top_words])\n",
        "\n",
        "df['vectorized']\n"
      ],
      "metadata": {
        "id": "zxl-JXoo-LQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Let's visualize the results"
      ],
      "metadata": {
        "id": "OD_MqVpg6gIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Flatten the list of lists of lemmatized words into a single list\n",
        "all_words = [word for sublist in df['vectorized'] for word in sublist]\n",
        "\n",
        "# Create a string of all words\n",
        "text = ' '.join(all_words)\n",
        "\n",
        "# Generate the word cloud\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "\n",
        "# Display the generated image:\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "86aIdHP8_kFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can you tell what is the common theme in the dataset?"
      ],
      "metadata": {
        "id": "ePK2-osz7Gc4"
      }
    }
  ]
}